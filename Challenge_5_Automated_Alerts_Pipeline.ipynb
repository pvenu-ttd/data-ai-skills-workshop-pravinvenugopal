{
  "cells": [
    {
      "cell_type": "code",
      "id": "4jRCin9y64ORATHd6UbkjRAB",
      "metadata": {
        "tags": [],
        "id": "4jRCin9y64ORATHd6UbkjRAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3f86d3ea-70dc-414f-b3e6-098c75d13903"
      },
      "source": [
        "# Install necessary libraries\n",
        "!pip install --upgrade google-cloud-storage google-cloud-bigquery google-cloud-aiplatform pandas requests"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (3.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.12/dist-packages (3.40.0)\n",
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.12/dist-packages (1.133.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.47.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.28.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (25.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (6.33.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.49.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.11.10)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (4.15.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.72.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.76.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.3)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.11.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import datetime\n",
        "from google.cloud import storage, bigquery\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from google.cloud.exceptions import Conflict, NotFound"
      ],
      "metadata": {
        "id": "l-Td45Z7PFIJ"
      },
      "id": "l-Td45Z7PFIJ",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "PROJECT_ID = \"qwiklabs-gcp-01-5f9c2ba2a4cd\"\n",
        "BUCKET_NAME = \"airport_alerts\"\n",
        "REGION = \"us\"\n",
        "MODEL_NAME = \"airport_alerts_model\"\n",
        "DATASET_ID = \"airport_weather\"\n",
        "TABLE_ID = \"weather_alerts\"\n",
        "SOURCE_CSV_URI = \"gs://labs.roitraining.com/data-to-ai-workshop/airports.csv\""
      ],
      "metadata": {
        "id": "hEkLmv-ePFFh"
      },
      "id": "hEkLmv-ePFFh",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Google Cloud Clients\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n"
      ],
      "metadata": {
        "id": "Vws9d2cvPFCG"
      },
      "id": "Vws9d2cvPFCG",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "export PROJECT_ID=$(gcloud config get-value project)\n",
        "export REGION=\"us\"\n",
        "export CONNECTION_ID=\"gemini-alert-conn\"\n",
        "\n",
        "# Enable necessary services\n",
        "gcloud services enable bigqueryconnection.googleapis.com aiplatform.googleapis.com --quiet\n",
        "\n",
        "# Check if the connection already exists\n",
        "if bq show --connection --location=$REGION $PROJECT_ID.$REGION.$CONNECTION_ID > /dev/null 2>&1; then\n",
        "    echo \"Connection '$CONNECTION_ID' already exists in $REGION. Skipping creation.\"\n",
        "else\n",
        "    echo \"Connection '$CONNECTION_ID' not found. Creating now...\"\n",
        "    bq mk --connection \\\n",
        "      --location=$REGION \\\n",
        "      --project_id=$PROJECT_ID \\\n",
        "      --connection_type=CLOUD_RESOURCE \\\n",
        "      $CONNECTION_ID\n",
        "fi\n",
        "\n",
        "# Retrieve the Service Account Email\n",
        "SA_EMAIL=$(bq show --format=json --connection $PROJECT_ID.$REGION.$CONNECTION_ID | jq -r '.cloudResource.serviceAccountId')\n",
        "\n",
        "echo \"Your Connection Service Account is: $SA_EMAIL\"\n",
        "\n",
        "# Bind the IAM Role\n",
        "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
        "    --member=\"serviceAccount:$SA_EMAIL\" \\\n",
        "    --role=\"roles/aiplatform.user\" \\\n",
        "    --condition=None --quiet > /dev/null\n",
        "\n",
        "echo \"Setup Complete.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "734UGWHhCY0k",
        "outputId": "4060469c-747f-4524-dbd7-c57332509678"
      },
      "id": "734UGWHhCY0k",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection 'gemini-alert-conn' already exists in us. Skipping creation.\n",
            "Your Connection Service Account is: bqcx-268999363694-i7bn@gcp-sa-bigquery-condel.iam.gserviceaccount.com\n",
            "Setup Complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Operation \"operations/acat.p2-268999363694-14fae2a2-3874-4155-862c-5865d8291a6b\" finished successfully.\n",
            "Updated IAM policy for project [qwiklabs-gcp-01-5f9c2ba2a4cd].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset if it doesn't exist\n",
        "dataset = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID}\")\n",
        "dataset.location = \"US\"\n",
        "\n",
        "try:\n",
        "    bq_client.create_dataset(dataset, exists_ok=True)\n",
        "    print(f\"Dataset {DATASET_ID} created.\")\n",
        "except Conflict:\n",
        "    print(f\"Dataset {DATASET_ID} already exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hunaYIbrPE-X",
        "outputId": "1781d240-3fc5-4ef5-972e-c490d86308ba"
      },
      "id": "hunaYIbrPE-X",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset airport_weather created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data into BigQuery and Fetch Large Aiports\n",
        "\n",
        "table_ref = f\"{PROJECT_ID}.{DATASET_ID}.airports\"\n",
        "\n",
        "try:\n",
        "    # Check if the table exists by attempting to fetch its metadata\n",
        "    bq_client.get_table(table_ref)\n",
        "    print(f\"Table {table_ref} already exists. Skipping load.\")\n",
        "\n",
        "except NotFound:\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        autodetect=True,\n",
        "    )\n",
        "\n",
        "    print(\"Loading airports data into BigQuery...\")\n",
        "    load_job = bq_client.load_table_from_uri(\n",
        "        SOURCE_CSV_URI,\n",
        "        table_ref,\n",
        "        job_config=job_config\n",
        "    )\n",
        "    load_job.result()\n",
        "    print(\"Load job finished.\")\n",
        "\n",
        "# Fetch Large Airports and Weather\n",
        "fetch_sql_query = f\"\"\"\n",
        "SELECT id, name, latitude_deg, longitude_deg\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.airports`\n",
        "WHERE type = 'large_airport' AND iso_country = 'US'\n",
        "LIMIT 100\n",
        "\"\"\"\n",
        "\n",
        "airports_df = bq_client.query(fetch_sql_query).to_dataframe()\n",
        "print(\"Fetched data from table.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVN39zUH1UNP",
        "outputId": "465c412f-ca46-4a99-80da-3d0689525912"
      },
      "id": "dVN39zUH1UNP",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table qwiklabs-gcp-01-5f9c2ba2a4cd.airport_weather.airports already exists. Skipping load.\n",
            "Fetched data from table.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGo-cArmVQWp"
      },
      "id": "iGo-cArmVQWp",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_airport_pipeline(bq_client, project_id, dataset_id, table_id, model_name, airports_df):\n",
        "    \"\"\"\n",
        "    Module: Fetches weather, stages data, and single-pass insert with gemini alert\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Gemini Model if it doesn't exist\n",
        "    create_model_sql = f\"\"\"\n",
        "    CREATE OR REPLACE MODEL `{dataset_id}.{model_name}`\n",
        "    REMOTE WITH CONNECTION DEFAULT\n",
        "    OPTIONS (endpoint = 'gemini-2.0-flash-001');\n",
        "    \"\"\"\n",
        "    print(\"Checking/Creating Gemini Model...\")\n",
        "    bq_client.query(create_model_sql).result()\n",
        "\n",
        "    results = []\n",
        "    headers = {'User-Agent': '(my-weather-app, contact@example.com)'}\n",
        "    target_table = f\"{project_id}.{dataset_id}.{table_id}\"\n",
        "    staging_table = f\"{target_table}_staging\"\n",
        "\n",
        "    # Fetch Raw Alert\n",
        "    print(f\"Fetching weather for {len(airports_df)} airports...\")\n",
        "    for _, row in airports_df.iterrows():\n",
        "        try:\n",
        "            # Get NWS Point & Forecast\n",
        "            point_url = f\"https://api.weather.gov/points/{row['latitude_deg']},{row['longitude_deg']}\"\n",
        "            point_res = requests.get(point_url, headers=headers).json()\n",
        "            forecast_url = point_res['properties']['forecast']\n",
        "\n",
        "            forecast_data = requests.get(forecast_url, headers=headers).json()\n",
        "            latest_forecast = forecast_data['properties']['periods'][0]['detailedForecast']\n",
        "\n",
        "            results.append({\n",
        "                \"id\": int(row['id']),\n",
        "                \"airport\": row['name'],\n",
        "                \"lat\": float(row['latitude_deg']),\n",
        "                \"lng\": float(row['longitude_deg']),\n",
        "                \"forecast\": latest_forecast\n",
        "            })\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if not results: return\n",
        "\n",
        "    # Create Base table\n",
        "    create_query = f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS `{project_id}.{dataset_id}.{table_id}`\n",
        "    (id INT64,\n",
        "    airport STRING,\n",
        "    lat FLOAT64,\n",
        "    lng FLOAT64,\n",
        "    forecast STRING,\n",
        "    gemini_weather_alert STRING,\n",
        "    last_updated TIMESTAMP);\n",
        "    \"\"\"\n",
        "\n",
        "    query_job = bq_client.query(create_query)\n",
        "    query_job.result()\n",
        "\n",
        "    # Load to Staging\n",
        "    staging_df = pd.DataFrame(results)\n",
        "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "    bq_client.load_table_from_dataframe(staging_df, staging_table, job_config=job_config).result()\n",
        "\n",
        "    # Merge and Gemini Alert\n",
        "    # Only generate alerts for the updates\n",
        "    merge_sql = f\"\"\"\n",
        "    MERGE `{target_table}` T\n",
        "    USING (\n",
        "      SELECT\n",
        "        id, airport, lat, lng, forecast, ml_generate_text_llm_result as ai_alert\n",
        "      FROM ML.GENERATE_TEXT(\n",
        "        MODEL `{project_id}.{dataset_id}.{model_name}`,\n",
        "        (\n",
        "          SELECT id, airport, lat, lng, forecast,\n",
        "          CONCAT(\n",
        "            \"Role: Aviation Weather Expert. Input: Forecast for \", airport, \": \", forecast, \". \",\n",
        "            \"Task: Return a brief 1-sentence alert. \",\n",
        "            \"Start with [RED] (Extreme/Hazardous), [AMBER] (Delays/Caution), or [GREEN] (Clear). \",\n",
        "            \"Use RED for severe storms/snow, AMBER for rain/high winds, GREEN for fair weather.\"\n",
        "            \"Return only the alert, nothing else.\"\n",
        "            ) as prompt\n",
        "          FROM `{staging_table}`\n",
        "        ),\n",
        "        STRUCT(0.2 AS temperature, TRUE AS flatten_json_output)\n",
        "      )\n",
        "    ) S\n",
        "    ON T.id = S.id AND S.forecast IS NOT NULL\n",
        "    WHEN MATCHED THEN\n",
        "      UPDATE SET\n",
        "        airport = S.airport,\n",
        "        lat = S.lat,\n",
        "        lng = S.lng,\n",
        "        forecast = S.forecast,\n",
        "        gemini_weather_alert = S.ai_alert,\n",
        "        last_updated = CURRENT_TIMESTAMP()\n",
        "    WHEN NOT MATCHED THEN\n",
        "      INSERT (id, airport, lat, lng, forecast, gemini_weather_alert, last_updated)\n",
        "      VALUES (S.id, S.airport, S.lat, S.lng, S.forecast, S.ai_alert, CURRENT_TIMESTAMP())\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Running Merge and Gemini Alerts...\")\n",
        "\n",
        "    try:\n",
        "      bq_client.query(merge_sql).result()\n",
        "      print(f\"Done. {len(results)} airports processed.\")\n",
        "\n",
        "      print(f\"Pipeline execution completed at (UTC): {datetime.datetime.now()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jw8l6JpsVCD2"
      },
      "id": "Jw8l6JpsVCD2",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization & Execution\n",
        "run_airport_pipeline(bq_client, PROJECT_ID, DATASET_ID, TABLE_ID, MODEL_NAME, airports_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLiljHsKVCBC",
        "outputId": "c17b2071-fe6b-4997-f516-df7bb23fcc0e"
      },
      "id": "qLiljHsKVCBC",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking/Creating Gemini Model...\n",
            "Fetching weather for 71 airports...\n",
            "Running Merge and Gemini Alerts...\n",
            "Done. 71 airports processed.\n",
            "Pipeline execution completed at (UTC): 2026-01-16 18:32:10.738029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vebh3IaUES8C"
      },
      "id": "vebh3IaUES8C",
      "execution_count": 114,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Challenge_5_Automated_Alerts_Pipeline"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}